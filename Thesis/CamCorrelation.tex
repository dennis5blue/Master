\subsection{Correlation Between Cameras}
We now describe how we define the correlation between cameras, which can be further
exploited to leverage spatial correlation while delivering data.
As shown in Figure~\ref{fig::multiCam}, suppose that camera $i$ and camera $j$ are
both observing the same object but from different positions and having different
sensing directions, there will cause some overlapped regions between the collected
views from the two cameras.
We thus divide the view of camera $i$ and $j$ into regions and experimentally
estimate if two regions from camera $i$ and $j$ are correlated.
This spatial correlation from neighborhood cameras can help to reduce the amount of
data needed to be delivered to the base station.
That is, if a region is not delivered to the base station, we can reconstruct it with the help
of images from neighboring cameras that are correlated to this missing region.
%
For example, in Figure~\ref{fig::multiCam}, images of the two cameras are divided into
$36$ regions, while some regions of camera $i$ only contain a white background, which
is similar to some regions of camera $j$.
Therefore, as long as one of those regions is transmitted, the remaining regions are no
longer necessary to be delivered to the base station.
However, some regions of camera $i$ and camera $j$ are unique so that those part must
be delivered to the aggregator for reconstructing images.
Therefore, in our work, we will determine a portion of regions of those cameras that are
required to be transmitted for the sake of reducing the power consumption of cameras
for data transmission.
\begin{figure}
\begin{center}
\includegraphics[width=0.95\columnwidth]{./Figures/multiCam}
\caption{\label{fig::multiCam}Correlation between cameras}
\end{center}
\end{figure}