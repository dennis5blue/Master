\subsection{Network Model}
\label{sec::dataModel}
Note that we discussed our target network scenario in chapter~\ref{sec::networkScenario}, therefore, in this chapter, we further introduce some related models that are used in this thesis.
%
%\subsubsection{Data Transmission Model}
%For each transmission link, to ensure the transmitted data $D$ can be successfully received, we require the allocated time $T$, bandwidth $W$, and link SINR $\gamma$ be sufficient as follows:
%\begin{equation}
%\label{eq::capaciy}
%TW\log_2\left(1+\frac{1}{\Gamma}\gamma\right) \geq D,
%\end{equation}
%where $\Gamma$ is a constant to model the gap between the achievable data rate of the selected modulation/coding scheme and the theoretic Shannon channel capacity~\cite{MQAM}.
%
\subsubsection{Camera Sensing Model}
\begin{figure}
\begin{center}
\includegraphics[width=0.95\columnwidth]{Figures/pinHoleCamera.pdf}
\caption{\label{fig::fov}Field of view of a camera}
\end{center}
\end{figure}
Different from scalar sensor networks, the multimedia data collected by cameras depends on the field of view (FoV) of the camera, where the field of view of a camera is highly correlated with the focal length and sensor size of the camera.
Suppose we only consider pin hole cameras in this thesis, the FoV of a camera can be illustrated as figure~\ref{fig::fov} shows.
That is, the FoV of a camera can be described by its sensing direction and its angular extent of a given scene, where the angular extent can be further defined as the angle of view $\theta$ of a camera.
Note that the angle of view (AoV) of a camera can be measured horizontally or vertically, however, we only present the horizontal measurement of AoV in this chapter, and the vertical measurement can be done by following the same idea.
For a $K$ pixels image, the light reflected by objects falling in the FoV of this camera will pass through the pin hole and be projected on the sensor with size $L$.
Since the sensor is apart from the pin hole with focal length $f$, the angle of view $\theta$ can thus be measured as:
\begin{equation}
\theta = 2 \arctan \frac{L}{2f}.
\label{eq::AoV}
\end{equation}
%
\subsubsection{Camera Correlation Model}
%
\begin{figure}
\centering
\includegraphics[width=0.95\columnwidth]{Figures/MVC}
\caption{Temporal and spatial prediction of multiview video coding\label{fig::MVC}}
\end{figure}
For the multi-camera networks, cameras are deployed for periodical collecting video streams around a neighboring area, and hence video streams produced by those cameras are often correlated due to the high deployment density.
The characteristics of the data correlation in multi-camera networks can be summarized as the following two phases:
\begin{itemize}
\item Temporal correlation: The video stream for the multi-camera networks is formed by taking a rectangular ``snapshot'' of the frames of each camera at periodic time intervals.
Therefore, successive frames may be correlated with each other in nature, and hence many existing video codec exploit the temporal correlation between frames to improve the coding efficiency.
The idea is to compensate the differences between frames by inter or motion compensated prediction.
A practical and widely used method of motion compensated prediction is to divide the current frame into rectangular sections or blocks and compensate the movement of those blocks.
The characteristic of temporal correlation for the multi-camera networks is well developed in the research area of video coding, therefore, we focus on leveraging the advantages of spatial correlation in this thesis.
\item Spatial correlation: Note that different cameras observing the same scene from different viewpoints will eventually produce correlated video streams.
Therefore, performing the spatial inter-view prediction is an important issue to be considered for the multi-camera networks especially when the cameras are densely deployed.
That is, under the multiview video coding scheme~\cite{MVCoverview}, a camera can not only reference from its previous frames but also can reference from the frames of nearby cameras as illustrated in figure~\ref{fig::MVC}.
We thus consider the spatial correlation between cameras in this thesis for the sake of improving the coding efficiency as well as reducing the amount of bits that are required to be transmitted.  
\end{itemize}
%

\begin{figure}
\begin{center}
\includegraphics[width=0.95\columnwidth]{./Figures/multiCam}
\caption{\label{fig::multiCam}Correlation between cameras}
\end{center}
\end{figure}
We now describe how we define the correlation between multiple cameras, which can be further exploited to leverage spatial correlation while delivering data.
As shown in Figure~\ref{fig::multiCam}, suppose that camera $i$ and camera $j$ are both observing the same object but from different positions and having different sensing directions, there will cause some overlapped regions between the collected views from the two cameras.
%We thus divide the view of camera $i$ and $j$ into regions and experimentally estimate if two regions from camera $i$ and $j$ are correlated.
This spatial correlation from neighboring cameras can help to reduce the amount of data needed to be delivered to the base station.
With the help of multi-view video encoding technique, correlated regions between two images can be encoded with less amount of bits, therefore, the usage of radio resource for communication can be reduced.
%That is, if a region is not delivered to the base station, we can reconstruct it with the help of images from neighboring cameras that are correlated to this missing region.
%
%For example, in Figure~\ref{fig::multiCam}, images of the two cameras are divided into $36$ regions, while some regions of camera $i$ only contain a white background, which is similar to some regions of camera $j$.
%Therefore, as long as one of those regions is transmitted, the remaining regions are no longer necessary to be delivered to the base station.
%However, some regions of camera $i$ and camera $j$ are unique so that those part must be delivered to the aggregator for reconstructing images.
%Therefore, in our work, we will determine a portion of regions of those cameras that are required to be transmitted for the sake of reducing the amount of transmission bits.
%

\begin{figure}
\begin{center}
\begin{subfigure}[b]{0.65\columnwidth}
\includegraphics[width=0.95\columnwidth]{Figures/overlappedCam.pdf}
\caption{\label{fig::overlappedCam}Overlapped field of view}
\end{subfigure}
\begin{subfigure}[b]{0.5\columnwidth}
\includegraphics[width=1.5\columnwidth]{Figures/calBaisedPixels.pdf}
\caption{\label{fig::calBaisedPixels}Biased pixels}
\end{subfigure}
\caption{\label{fig::overlappedAndBaised}Illustration of overlapped field of view and biased pixels}
\end{center}
\end{figure}
%
However, it rises a problem that how to determine the search range of multi-view video encoder.
It is clearly that the correlation level between two cameras highly depends on the area of their overlapped FoV.
As shown in figure~\ref{fig::overlappedCam}, if camera $1$ and camera $2$ are observing the same area at the same position with the same image resolution $K$ (pixels) but having a difference of sensing direction equaled to $\phi$.
Then we can claim that the image collected from camera $2$ is just a ``shift'' of camera $1$'s image, where we denote shifted extent as the amount of biased pixels $\kappa$.
The calculation of biased pixels is demonstrated in figure~\ref{fig::calBaisedPixels}, where the two rectangular triangles share the same base-side, that is,
\begin{equation}
\frac{\frac{K}{2}}{\tan \frac{\theta}{2}} = \frac{\frac{K}{2}-\kappa}{\tan(\frac{\theta}{2}-\phi)}.
\label{eq::calBiasedPixels}
\end{equation}
Therefore, the amount of biased pixels is
\begin{equation}
\kappa = \frac{K}{2} - \frac{\frac{K}{2} \tan(\frac{\theta}{2}-\phi) }{\tan \frac{\theta}{2}},
\label{eq::biasedPixels}
\end{equation}
and the value of $\kappa$ can be used when performing the motion estimation procedure of multi-view encoding technique.
%
\subsection{Modified Motion Estimation Technique}
%
\begin{figure}
\begin{subfigure}[b]{\columnwidth}
\begin{center}
\includegraphics[width=0.95\columnwidth]{Figures/motionEstimation.pdf}
\caption{\label{fig::modifiedME}Demonstration of conventional motion estimation technique}
\end{center}
\end{subfigure}
%
\begin{subfigure}[b]{\columnwidth}
\begin{center}
\includegraphics[width=0.95\columnwidth]{Figures/modifiedMotionEstimation.pdf}
\caption{\label{fig::modifiedME}Demonstration of modified motion estimation technique}
\end{center}
\end{subfigure}
\end{figure}
As we mentioned in the previous chapter, when two cameras are observing the same area but having different sensing direction, then the collected image might just be a shifted image from the other one.
Therefore, it motivates us to modify the conventional motion estimation technique so that we can have a larger possibility to find a correlated macroblock under a given search range.
The idea of our modified motion estimation technique is shown in figure~\ref{fig::modifiedME}, where we shift the search region by the value $\kappa$ estimated from equation~\eqref{eq::biasedPixels}.