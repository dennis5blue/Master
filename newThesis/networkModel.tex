\subsection{Camera Model}
\label{sec::dataModel}
Note that we finish discussing our target network scenario in Chapter~\ref{sec::networkScenario}, therefore, in this chapter, we further introduce some related models that are used in this thesis.
%
%\subsubsection{Data Transmission Model}
%For each transmission link, to ensure the transmitted data $D$ can be successfully received, we require the allocated time $T$, bandwidth $W$, and link SINR $\gamma$ be sufficient as follows:
%\begin{equation}
%\label{eq::capaciy}
%TW\log_2\left(1+\frac{1}{\Gamma}\gamma\right) \geq D,
%\end{equation}
%where $\Gamma$ is a constant to model the gap between the achievable data rate of the selected modulation/coding scheme and the theoretic Shannon channel capacity~\cite{MQAM}.
%
\subsubsection{Camera Sensing Model}
\begin{figure}
\begin{center}
\includegraphics[width=0.95\columnwidth]{Figures/pinHoleCamera.pdf}
\caption{\label{fig::fov}Field of view of a camera}
\end{center}
\end{figure}
Different from scalar sensor networks, the multimedia data collected by cameras depends on the field of view (FoV) of the camera.
\begin{mydef}[\textbf{Field of View}]
In photography, a field of view (FoV)~\cite{szeliski2010computer} is the extent of the observable world that is seen by a camera at any time instance, which is characterised by the angle of view of this camera.
\label{def::FoV}
\end{mydef}
\begin{mydef}[\textbf{Angle of View}]
In photography, an angle of view (AoV) $\theta$~\cite{szeliski2010computer} it the angular extent of a given scene that is rendered by a camera.
In general, the AoV of a camera depends on the focal length $f$ and sensor size $L$ of this camera, and it should obey the following formula:
\begin{equation}
\tan \frac{\theta}{2} = \frac{L}{2f}.
\label{eq::AoVDefinition}
\end{equation}
\label{def::AoV}
\end{mydef}
Suppose we only consider pin hole cameras in this thesis, the FoV of a camera can be illustrated as Fig.~\ref{fig::fov} shows.
That is, the FoV of a camera can be described by its sensing direction and its angular extent of a given scene, where the angular extent is the angle of view $\theta$ of this camera as defined in definition~\ref{def::AoV}.
Note that the angle of view (AoV) of a camera can be measured horizontally or vertically, however, we only present the horizontal measurement of AoV in this chapter, and the vertical measurement can be done by following the same idea.
For a $K$ pixels image, the light reflected by objects falling in the FoV of this camera will pass through the pin hole and be projected on the sensor with size $L$.
Since the sensor is apart from the pin hole with focal length $f$, by equation~\eqref{eq::AoVDefinition}, the angle of view $\theta$ can thus be measured as:
\begin{equation}
\theta = 2 \arctan \frac{L}{2f},
\label{eq::AoV}
\end{equation}
and the region span by an angular extent $\theta$ is the FoV of this pin hole camera.
%
\subsubsection{Camera Correlation Model}
%
For the multi-camera networks, cameras are deployed for periodical collecting video streams around a neighboring area, and hence video streams produced by those cameras are often correlated due to the high deployment density.
The characteristics of the data correlation in multi-camera networks can be summarized as the following two phases:
\begin{itemize}
\item Temporal correlation: The video stream for the multi-camera networks is formed by taking a rectangular ``snapshot'' of the frames of each camera at periodic time intervals.
Therefore, successive frames may be correlated with each other in nature, and hence many existing H.264 video codec exploit the temporal correlation between frames to improve the coding efficiency.
The idea is to compensate the differences between frames by inter or motion compensated prediction as described in Chapter~\ref{sec::H264CompressionIntro}.
%A practical and widely used method of motion compensated prediction is to divide the current frame into rectangular sections or blocks and compensate the movement of those blocks.
The characteristic of temporal correlation for the multi-camera networks is well developed in the research area of video encoding, therefore, we focus on leveraging the advantages of spatial correlation in this thesis.
\item Spatial correlation: Note that different cameras observing the same scene from different viewpoints will eventually produce correlated video streams.
Therefore, performing the spatial inter-view prediction is an important issue to be considered for the multi-camera networks especially when the cameras are densely deployed.
That is, by exploiting the MVC scheme as presented in Chapter~\ref{sec::MultiviewIntro}, a camera can not only reference from its previous frames but also can reference from the frames of nearby cameras.
We thus consider the spatial correlation between cameras in this thesis for the sake of improving the coding efficiency as well as reducing the amount of bits that are required to be transmitted.  
\end{itemize}
%
We now describe how we define the correlation between multiple cameras, which can be further exploited to leverage spatial correlation while delivering data.
As shown in Fig.~\ref{fig::multiCam}, suppose that camera $i$ and camera $j$ are both observing the same object but from different positions and having different sensing directions, there will cause some overlapped regions between the collected views from the two cameras.
This spatial correlation from neighboring cameras can help to reduce the amount of data needed to be delivered to the base station.
With the help of multi-view video encoding technique, correlated regions between two images can be encoded with less amount of bits, therefore, the usage of radio resource for communication can be reduced.

\begin{figure}
\begin{center}
\begin{subfigure}[b]{0.65\columnwidth}
\includegraphics[width=0.95\columnwidth]{Figures/overlappedCam.pdf}
\caption{\label{fig::overlappedCam}Overlapped field of view}
\end{subfigure}
\begin{subfigure}[b]{0.5\columnwidth}
\includegraphics[width=1.5\columnwidth]{Figures/calBaisedPixels.pdf}
\caption{\label{fig::calBaisedPixels}Biased pixels}
\end{subfigure}
\caption{\label{fig::overlappedAndBaised}Illustration of overlapped field of view and biased pixels}
\end{center}
\end{figure}
%
However, it rises a problem that how to determine the search range of multi-view video encoder.
It is clearly that the correlation level between two cameras highly depends on the area of their overlapped FoV.
As shown in Fig.~\ref{fig::overlappedCam}, if camera $1$ and camera $2$ are observing the same area at the same position with the same image resolution $K$ (pixels) but having a difference of sensing direction equaled to $\phi$.
Then we can claim that the image collected from camera $2$ is just a ``shift'' of camera $1$'s image, where we denote shifted extent as the amount of biased pixels $\kappa$.
The calculation of biased pixels is demonstrated in Fig.~\ref{fig::calBaisedPixels}, where the two rectangular triangles share the same base-side, that is,
\begin{equation}
\frac{\frac{K}{2}}{\tan \frac{\theta}{2}} = \frac{\frac{K}{2}-\kappa}{\tan(\frac{\theta}{2}-\phi)}.
\label{eq::calBiasedPixels}
\end{equation}
Therefore, the amount of biased pixels is
\begin{equation}
\kappa = \frac{K}{2} - \frac{\frac{K}{2} \tan(\frac{\theta}{2}-\phi) }{\tan \frac{\theta}{2}},
\label{eq::biasedPixels}
\end{equation}
and the value of $\kappa$ can be used when performing the motion estimation procedure of multi-view encoding technique.
%